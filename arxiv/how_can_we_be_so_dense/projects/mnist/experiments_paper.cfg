# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

#
# Parameters used in the paper
#

[DEFAULT]
# Keep final results in a separate location
path = results

experiment = grid
repetitions = 1
; Uncomment to average over multiple seeds
;repetitions = 10

seed = 42
datadir = data
lr_scheduler = StepLR
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"
validation = 1.0
no_cuda = False
batches_in_epoch = 100000
log_interval = 1000
test_batch_size = 1000
create_plots = False
count_nonzeros = True
test_noise_every_epoch = True

use_batch_norm = False
boost_strength_factor = 1.0
optimizer = SGD

c1_input_shape = "1_28_28"
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the main networks described in the MNIST section
; of the paper.


[denseCNN1]
c1_out_channels = "30"
c1_k = "4320"
n = 1000
k = 1000
iterations = 15
boost_strength = 0.0
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.9
weight_sparsity = 1.0
k_inference_factor = 1.5
use_cnn = True
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = True
create_plots = False
batch_size = 16
validation = 1.0
saveNet = True

[denseCNN2]
c1_out_channels = "30_30"
c1_k = "4320_480"
n = 1000
k = 1000
iterations = 15
boost_strength = 0.0
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.9
weight_sparsity = 1.0
k_inference_factor = 1.5
use_cnn = True
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = True
create_plots = False
batch_size = 16
validation = 1.0
saveNet = True

[sparseCNN1]
c1_out_channels = 30
c1_k = 400
n = 150
k = 50
iterations = 20
boost_strength = 1.4
boost_strength_factor = 0.85
learning_rate = 0.02
momentum = 0.0
learning_rate_factor = 0.7
weight_sparsity = 0.3
k_inference_factor = 1.5
use_cnn = True
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = True
create_plots = False
batch_size = 4
validation = 1.0
saveNet = True


[sparseCNN2]
c1_out_channels = "30_30"
c1_k = "400_400"
n = 300
k = 50
iterations = 20
boost_strength = 1.5
boost_strength_factor = 0.85
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.0
weight_sparsity = 0.3
k_inference_factor = 1.5
use_cnn = True
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = True
create_plots = False
batch_size = 4
validation = 1.0
use_batch_norm = False
saveNet = True


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the mixed networks described in the MNIST section
; of the paper.

; Dense CNN layers with a sparse hidden layer identical to Sparse CNN2
[denseCNN2SP3]
c1_out_channels = "30_30"
c1_k = "4320_480"
n = 300
k = 50
iterations = 15
boost_strength = 1.5
boost_strength_factor = 0.85
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.5
weight_sparsity = 0.4
k_inference_factor = 1.5
use_cnn = True
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = True
create_plots = False
batch_size = 4
validation = 1.0
saveNet = False

; Sparse CNN layers with a dense hidden layer like Dense CNN2
[sparseCNN2D3]
c1_out_channels = "30_30"
c1_k = "400_400"
n = 1000
k = 1000
iterations = 15
boost_strength = 1.5
boost_strength_factor = 0.85
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.9
weight_sparsity = 1.0
k_inference_factor = 1.5
use_cnn = True
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = True
create_plots = False
batch_size = 16
validation = 1.0
use_batch_norm = False
saveNet = False

; Same as Sparse CNN-2 except the hidden layer has weight sparsity = 1
[sparseCNN2W1]
c1_out_channels = "30_30"
c1_k = "400_400"
n = 300
k = 50
iterations = 15
boost_strength = 1.5
boost_strength_factor = 0.85
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.9
weight_sparsity = 1.0
k_inference_factor = 1.5
use_cnn = True
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = True
create_plots = False
batch_size = 16
validation = 1.0
use_batch_norm = False
saveNet = False

; Sparse CNN-2 with a hidden layer like Dense CNN-2 but with weight sparsity 0.3
[sparseCNN2DSW]
c1_out_channels = "30_30"
c1_k = "400_400"
n = 1000
k = 1000
iterations = 15
boost_strength = 1.5
boost_strength_factor = 0.85
learning_rate = 0.01
learning_rate_factor = 0.8
momentum = 0.9
weight_sparsity = 0.3
k_inference_factor = 1.5
use_cnn = True
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = True
create_plots = False
batch_size = 16
validation = 1.0
use_batch_norm = False
saveNet = False
