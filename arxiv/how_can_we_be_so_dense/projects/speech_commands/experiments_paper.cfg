# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------
[DEFAULT]

repetitions = 1
; Uncomment to average over multiple seeds
;repetitions = 10

iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
weight_decay = 0.01
learning_rate_factor = 1.0
use_batch_norm = True
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 2000
k = 200
weight_sparsity = 0.50
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = False # If False, will only test noise at end

path = results
datadir = data
background_noise_dir = _background_noise_

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = StepLR

model_type = linear  # cnn, resnet9, or linear
c1_out_channels = 10
c1_k = 6
dropout = 0.5

run_noise_tests = True
count_nonzeros = False
save_every_epoch = True

c1_input_shape = "1_32_32"


[sparseCNN2]
n = 1000
k = 100
c1_out_channels = 64_64
c1_k = 1200_200
k_inference_factor = 1.5
iterations = 30
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
weight_sparsity = 0.4
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = cnn

[denseCNN2]
n = 1000
k = 1000
c1_out_channels = 64_64
c1_k = 12544_1600
k_inference_factor = 1.5
iterations = 20
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.8
learning_rate = 0.01
momentum = 0.9
weight_sparsity = 1.0
dropout = [0.0, 0.5]
log_interval = 100
batches_in_epoch = 5121
batch_size = 64
model_type = cnn
use_batch_norm = True


; Larger sparse with lower weight sparsity
[SuperSparseCNN2]
n = 1500
k = 100
c1_out_channels = 64_64
c1_k = 1200_200
k_inference_factor = 1.5
iterations = 25
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
weight_sparsity = 0.1
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = cnn

